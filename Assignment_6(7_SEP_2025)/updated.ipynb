{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be3a9716",
   "metadata": {},
   "source": [
    "# Updated NER with DeBERTa (BIO tags, seqeval)\n",
    "This notebook provides an updated, reproducible pipeline to fine-tune a DeBERTa model for token classification (NER) using the PII detection dataset. It includes:\n",
    "\n",
    "- Installation of required libraries,\n",
    "- Loading the dataset from specified paths,\n",
    "- Preprocessing labels to BIO format and aligning to subtokens (with -100 for subtokens),\n",
    "- Fine-tuning a DeBERTa model using Hugging Face Trainer with seqeval metric,\n",
    "- Inference demonstration using transformers.pipeline with aggregation strategy.\n",
    "\n",
    "Ensure dataset files are available at the specified paths or update DATA_ROOTS accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bec7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install -q transformers[torch] datasets accelerate evaluate seqeval wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6a2608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and configuration\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForTokenClassification\n",
    "from transformers import DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "\n",
    "# Dataset paths\n",
    "DATA_ROOTS = [\n",
    "    '/kaggle/input/pii-detection-removal-from-educational-data',\n",
    "    './data',\n",
    "]\n",
    "\n",
    "def find_file(name):\n",
    "    for root in DATA_ROOTS:\n",
    "        p = Path(root) / name\n",
    "        if p.exists():\n",
    "            return str(p)\n",
    "    return None\n",
    "\n",
    "TRAIN_FILE = find_file('train.json')\n",
    "TEST_FILE = find_file('test.json')\n",
    "SAMPLE_SUB = find_file('sample_submission.csv')\n",
    "\n",
    "print('Train file:', TRAIN_FILE)\n",
    "print('Test file:', TEST_FILE)\n",
    "print('Sample submission:', SAMPLE_SUB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a211426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "if TRAIN_FILE:\n",
    "    ds = load_dataset('json', data_files={'train': TRAIN_FILE, 'test': TEST_FILE} if TEST_FILE else {'train': TRAIN_FILE})\n",
    "    print(ds)\n",
    "else:\n",
    "    print('Dataset files not found. Please update DATA_ROOTS or place files in ./data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea45e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect dataset examples\n",
    "if 'ds' in locals():\n",
    "    for i in range(min(3, len(ds['train']))):\n",
    "        example = ds['train'][i]\n",
    "        print('Example:')\n",
    "        for key, value in example.items():\n",
    "            print(f'{key}: {type(value)}')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fa36ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for preprocessing\n",
    "def whitespace_tokenize_with_spans(text):\n",
    "    tokens = []\n",
    "    spans = []\n",
    "    i = 0\n",
    "    for tok in text.split():\n",
    "        j = text.find(tok, i)\n",
    "        if j == -1:\n",
    "            j = i\n",
    "        tokens.append(tok)\n",
    "        spans.append((j, j + len(tok)))\n",
    "        i = j + len(tok)\n",
    "    return tokens, spans\n",
    "\n",
    "def spans_to_bio(tokens, token_spans, entities):\n",
    "    labels = ['O'] * len(tokens)\n",
    "    for ent in entities:\n",
    "        start, end, lab = ent['start'], ent['end'], ent.get('label', ent.get('entity'))\n",
    "        first = True\n",
    "        for i, (ts, te) in enumerate(token_spans):\n",
    "            if te <= start or ts >= end:\n",
    "                continue\n",
    "            prefix = 'B-' if first else 'I-'\n",
    "            labels[i] = prefix + str(lab)\n",
    "            first = False\n",
    "    return labels\n",
    "\n",
    "def align_labels_to_tokens(tokenizer, words, labels, label_to_id, label_all_tokens=False):\n",
    "    encoding = tokenizer(words, is_split_into_words=True, truncation=True, padding=False)\n",
    "    word_ids = encoding.word_ids()\n",
    "    aligned_labels = []\n",
    "    previous_word_idx = None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            aligned_labels.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            aligned_labels.append(label_to_id[labels[word_idx]])\n",
    "        else:\n",
    "            aligned_labels.append(label_to_id[labels[word_idx]] if label_all_tokens else -100)\n",
    "        previous_word_idx = word_idx\n",
    "    encoding['labels'] = aligned_labels\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcee016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer and build dataset\n",
    "MODEL_NAME = 'microsoft/deberta-v3-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "tokenizer.model_max_length = 512\n",
    "\n",
    "if 'ds' in locals():\n",
    "    tokenized_examples, label_list, label_to_id = build_token_classification_dataset(ds, tokenizer)\n",
    "    print('Labels:', label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2a424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setup\n",
    "if 'label_list' in locals():\n",
    "    id2label = {i: l for i, l in enumerate(label_list)}\n",
    "    label2id = {l: i for i, l in enumerate(label_list)}\n",
    "    config = AutoConfig.from_pretrained(MODEL_NAME, num_labels=len(label_list), id2label=id2label, label2id=label2id)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "    \n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "    \n",
    "    from datasets import Dataset\n",
    "    train_ds = Dataset.from_dict(tokenized_examples)\n",
    "    dataset_for_trainer = DatasetDict({'train': train_ds})\n",
    "    \n",
    "    seqeval = evaluate.load('seqeval')\n",
    "    \n",
    "    def compute_metrics(p):\n",
    "        predictions, labels = p\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "        true_predictions = [\n",
    "            [label_list[pred] for (pred, lab) in zip(prediction, label) if lab != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "        true_labels = [\n",
    "            [label_list[lab] for (pred, lab) in zip(prediction, label) if lab != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "        results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "        return {\n",
    "            'precision': results['overall_precision'],\n",
    "            'recall': results['overall_recall'],\n",
    "            'f1': results['overall_f1'],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fed28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "if 'model' in locals():\n",
    "    import torch\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./deberta-ner-output',\n",
    "        eval_strategy='no',\n",
    "        learning_rate=3e-5,\n",
    "        per_device_train_batch_size=1,  # Minimal batch size for GPU memory\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type='cosine',\n",
    "        save_total_limit=1,\n",
    "        fp16=True,  # Enable fp16 for GPU\n",
    "        report_to='none',\n",
    "        gradient_checkpointing=True,  # Re-enable for memory savings\n",
    "        dataloader_num_workers=0,\n",
    "        logging_steps=50,\n",
    "        save_steps=200,\n",
    "        optim='adamw_torch',\n",
    "        # Remove no_cuda=True to use GPU\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset_for_trainer['train'],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    # Uncomment to train\n",
    "    trainer.train()\n",
    "    trainer.save_model('./deberta-ner-final')\n",
    "    print('Training completed and model saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d108f9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference with pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "model_dir = './deberta-ner-final' if Path('./deberta-ner-final').exists() else MODEL_NAME\n",
    "ner_pipe = pipeline('ner', model=model_dir, tokenizer=tokenizer, aggregation_strategy='simple')\n",
    "\n",
    "text = 'Contact John Doe at john.doe@example.com or call +1 555-555-5555.'\n",
    "entities = ner_pipe(text)\n",
    "print('Aggregated Entities:')\n",
    "for e in entities:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
